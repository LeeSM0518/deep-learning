{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence (구글 번역의 신경망 모델)\n",
    "* 순차적인 정보를 입력받는 신경망(RNN)과 출력하는 신경망을 조합한 모델로, 번역이나 챗봇 등 문장을 입력받아 다른 문자을 출력하는 프로그램에서 많이 사용.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Seqeuence to Sequence 모델\n",
    "<img src=\"https://sunghan-kim.github.io/assets/images/book/3minDL/ch10-05-seq2seq-concept.jpg\">\n",
    "\n",
    "* 입력을 위한 신경망인 인코더와 출력을 위한 신경망인 디코더로 구성된다.\n",
    "* **go** : 디코더에 입력이 시작됨을 알려줌\n",
    "\n",
    "* **eos** : 디코더에 출력이 끝났음을 알려줌\n",
    "\n",
    "* **인코더로 원문을 받으면 디코더로 부터 번역한 결과물을 받는다**\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 네 글자의 영어 단어를 입력받아 두 글자의 한글 단어로 번역하는 프로그램\n",
    "\n",
    "<img src=\"https://sunghan-kim.github.io/assets/images/book/3minDL/ch10-06-seq2seq-translate-model.jpg\">\n",
    "\n",
    "* **<?>** : 심볼\n",
    "  * **S** : 디코더의 시작\n",
    "  * **E** : 디코더의 끝\n",
    "  * **P** : 아무 의미 없는 심볼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 데이터 만들기\n",
    "1. 단어를 원-핫 인코딩 형식으로 바꿔야 하므로 char_arr 에 글자들을 나열한다.\n",
    "2. 한 글자, 한 글자의 인덱스 값을 지정해서 딕셔너리 형태로 num_dic에 저장한다.\n",
    "3. 글자개수를 dic_len에 저장\n",
    "4. 학습할 단어들 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "seq_data = [['word', '단어'], ['wood', '나무'],\n",
    "           ['game', '놀이'], ['girl', '소녀'],\n",
    "           ['kiss', '키스'], ['love', '사랑']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 함수 생성\n",
    "* **입력 단어와 출력 단어를 매개변수로 받아 인코더의 입력값, 디코더의 입력값과 출력값을 반환하는 함수 생성**\n",
    "  1. 인코더 셀의 입력값을 위해 입력 단어를 한 글자씩 떼어 배열로 만든다.\n",
    "  2. 디코더 셀의 입력값을 위해 출력 단어의 글자들을 배열로 만들고, 시작을 나타내는 심볼 'S'를 맨 앞에 붇인다.\n",
    "  3. 학습을 위해 비교할 디코더 셀의 출력값을 만들고, 출력의 끝을 알려주는 심볼 'E'를 마지막에 붙인다.\n",
    "  4. 만든 데이터들을 원-핫 인코딩한다.\n",
    "  \n",
    "> 손실 함수가 실측값을 원-핫 인코딩한 형태로 자동 변환하여 계산해주기 때문에 디코더 출력값은 인덱스 값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [25, 17, 20, 6]\n",
      "output:  [0, 29, 30]\n",
      "target:  [29, 30, 1]\n",
      "\n",
      "input one-hot:  [array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]])] \n",
      "\n",
      "output one-hot:  [array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.]])] \n",
      "\n",
      "target index data:  [[29, 30, 1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예시\n",
    "test_data = ['word', '단어']\n",
    "test_input = []\n",
    "test_output = []\n",
    "test_target = []\n",
    "\n",
    "input = [num_dic[n] for n in test_data[0]]\n",
    "print('input: ', input)\n",
    "\n",
    "output = [num_dic[n] for n in ('S' + test_data[1])]\n",
    "print('output: ', output)\n",
    "\n",
    "target = [num_dic[n] for n in (test_data[1] + 'E')]\n",
    "print('target: ', target)\n",
    "\n",
    "test_input.append(np.eye(dic_len)[input])    \n",
    "test_output.append(np.eye(dic_len)[output])\n",
    "test_target.append(target)\n",
    "\n",
    "print()\n",
    "print('input one-hot: ', test_input, '\\n')\n",
    "print('output one-hot: ', test_output, '\\n')\n",
    "print('target index data: ', test_target, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 신경망 모델을 구성할 변수들 정의\n",
    "* 하이퍼파라미터, 입출력 변수용 수치 정의\n",
    "* n_class 와 n_input은 입출력에 사용할 글자들의 배열 크기인 dic_len(글자수)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "n_class = n_input = dic_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 신경망 모델 구성\n",
    "* **플레이스홀더 정의**\n",
    "  * **인코더 입력 플레이스홀더, 디코더의 입력 플레이스홀더** : 원-핫 인코딩을 입력으로 사용\n",
    "  * **디코더 출력 플레이스홀더** : 인덱스 숫자를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## RNN 모델을 위한 셀 구성\n",
    "* **주의할 점** : 디코더를 만들 때 디코더의 초기 상태 값으로 인코더의 최종 상태 값을 넣어줘야 한다. 인코더에서 계산한 상태를 디코더로 전파하기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0801 12:48:35.089006 4658525632 deprecation.py:323] From <ipython-input-17-0aedb3fe390b>:2: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0801 12:48:35.093466 4658525632 deprecation.py:323] From <ipython-input-17-0aedb3fe390b>:7: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0801 12:48:35.145265 4658525632 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0801 12:48:35.153203 4658525632 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell,\n",
    "                                            output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                           dtype=tf.float32)\n",
    "    \n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell,\n",
    "                                            output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                           initial_state=enc_states, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 출력층, 손실 함수, 최적화 함수 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 12:52:29.377341 4658525632 deprecation.py:323] From <ipython-input-19-ea2126ae9a2e>:1: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model, labels=targets))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 학습\n",
    "* **feed_dict**\n",
    "  * 인코더의 입력값, 디코더의 입력값과 출력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  3.770413\n",
      "Epoch:  0002 cost =  2.841233\n",
      "Epoch:  0003 cost =  1.632053\n",
      "Epoch:  0004 cost =  1.204901\n",
      "Epoch:  0005 cost =  0.915784\n",
      "Epoch:  0006 cost =  0.429960\n",
      "Epoch:  0007 cost =  0.416665\n",
      "Epoch:  0008 cost =  0.234133\n",
      "Epoch:  0009 cost =  0.275301\n",
      "Epoch:  0010 cost =  0.157170\n",
      "Epoch:  0011 cost =  0.094532\n",
      "Epoch:  0012 cost =  0.078004\n",
      "Epoch:  0013 cost =  0.039385\n",
      "Epoch:  0014 cost =  0.051178\n",
      "Epoch:  0015 cost =  0.064218\n",
      "Epoch:  0016 cost =  0.047166\n",
      "Epoch:  0017 cost =  0.010996\n",
      "Epoch:  0018 cost =  0.017458\n",
      "Epoch:  0019 cost =  0.022575\n",
      "Epoch:  0020 cost =  0.103357\n",
      "Epoch:  0021 cost =  0.003816\n",
      "Epoch:  0022 cost =  0.010939\n",
      "Epoch:  0023 cost =  0.007844\n",
      "Epoch:  0024 cost =  0.009583\n",
      "Epoch:  0025 cost =  0.002845\n",
      "Epoch:  0026 cost =  0.001567\n",
      "Epoch:  0027 cost =  0.004725\n",
      "Epoch:  0028 cost =  0.003765\n",
      "Epoch:  0029 cost =  0.006425\n",
      "Epoch:  0030 cost =  0.008225\n",
      "Epoch:  0031 cost =  0.001264\n",
      "Epoch:  0032 cost =  0.006487\n",
      "Epoch:  0033 cost =  0.004543\n",
      "Epoch:  0034 cost =  0.002808\n",
      "Epoch:  0035 cost =  0.001935\n",
      "Epoch:  0036 cost =  0.003001\n",
      "Epoch:  0037 cost =  0.001333\n",
      "Epoch:  0038 cost =  0.001750\n",
      "Epoch:  0039 cost =  0.006115\n",
      "Epoch:  0040 cost =  0.002202\n",
      "Epoch:  0041 cost =  0.002087\n",
      "Epoch:  0042 cost =  0.001882\n",
      "Epoch:  0043 cost =  0.001415\n",
      "Epoch:  0044 cost =  0.001298\n",
      "Epoch:  0045 cost =  0.000452\n",
      "Epoch:  0046 cost =  0.001707\n",
      "Epoch:  0047 cost =  0.000984\n",
      "Epoch:  0048 cost =  0.001040\n",
      "Epoch:  0049 cost =  0.000633\n",
      "Epoch:  0050 cost =  0.002156\n",
      "Epoch:  0051 cost =  0.000522\n",
      "Epoch:  0052 cost =  0.001893\n",
      "Epoch:  0053 cost =  0.001277\n",
      "Epoch:  0054 cost =  0.000578\n",
      "Epoch:  0055 cost =  0.003057\n",
      "Epoch:  0056 cost =  0.000883\n",
      "Epoch:  0057 cost =  0.001128\n",
      "Epoch:  0058 cost =  0.000787\n",
      "Epoch:  0059 cost =  0.000377\n",
      "Epoch:  0060 cost =  0.000626\n",
      "Epoch:  0061 cost =  0.000808\n",
      "Epoch:  0062 cost =  0.001378\n",
      "Epoch:  0063 cost =  0.000781\n",
      "Epoch:  0064 cost =  0.000565\n",
      "Epoch:  0065 cost =  0.000952\n",
      "Epoch:  0066 cost =  0.000648\n",
      "Epoch:  0067 cost =  0.001121\n",
      "Epoch:  0068 cost =  0.001620\n",
      "Epoch:  0069 cost =  0.001005\n",
      "Epoch:  0070 cost =  0.001754\n",
      "Epoch:  0071 cost =  0.000503\n",
      "Epoch:  0072 cost =  0.002210\n",
      "Epoch:  0073 cost =  0.000423\n",
      "Epoch:  0074 cost =  0.000878\n",
      "Epoch:  0075 cost =  0.000434\n",
      "Epoch:  0076 cost =  0.000220\n",
      "Epoch:  0077 cost =  0.000493\n",
      "Epoch:  0078 cost =  0.000275\n",
      "Epoch:  0079 cost =  0.000925\n",
      "Epoch:  0080 cost =  0.000217\n",
      "Epoch:  0081 cost =  0.000481\n",
      "Epoch:  0082 cost =  0.000396\n",
      "Epoch:  0083 cost =  0.000307\n",
      "Epoch:  0084 cost =  0.001159\n",
      "Epoch:  0085 cost =  0.000834\n",
      "Epoch:  0086 cost =  0.000182\n",
      "Epoch:  0087 cost =  0.000457\n",
      "Epoch:  0088 cost =  0.000431\n",
      "Epoch:  0089 cost =  0.000251\n",
      "Epoch:  0090 cost =  0.000712\n",
      "Epoch:  0091 cost =  0.000262\n",
      "Epoch:  0092 cost =  0.000129\n",
      "Epoch:  0093 cost =  0.000358\n",
      "Epoch:  0094 cost =  0.000972\n",
      "Epoch:  0095 cost =  0.000423\n",
      "Epoch:  0096 cost =  0.000877\n",
      "Epoch:  0097 cost =  0.000415\n",
      "Epoch:  0098 cost =  0.000157\n",
      "Epoch:  0099 cost =  0.000490\n",
      "Epoch:  0100 cost =  0.000255\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "sess  = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                          feed_dict={enc_input: input_batch,\n",
    "                                    dec_input: output_batch,\n",
    "                                    targets: target_batch})\n",
    "    print('Epoch: ', '%04d' %(epoch + 1),\n",
    "         'cost = ', '{:.6f}'.format(loss))\n",
    "    \n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 단어 예측 함수 구현\n",
    "1. 이 모델은 입력값과 출력값 데이터로 (영어 단어, 한글 단어)를 사용하지만 예측 시에는 한글 단어를 알지 못한다. 즉, 단어를 예측하는 것이기 때문에 예측할 단어를 아직 모른다는 의미이다. 따라서 디코더의 입출력을 의미 없는 심볼인 'P'로 채워 넣는다.\n",
    "  * **ex) ['word', '나무'] 이면 ['word', 'PPPP'] 로 구성이 된다.** \n",
    "  \n",
    "  <br>\n",
    "  \n",
    "2. argmax 함수를 통해 가장 확률이 높은 글자의 인덱스들을 추출해 예측값으로 만든다\n",
    "  * **ex) [[[0, 0.9, 0.1, ...], [0, 0.1, 0.7, ...], ...], ...] => [[[1], [2], ...]]**\n",
    "  \n",
    "  <br>\n",
    "  \n",
    "3. 예측 결과는 글자의 인덱스를 뜻하기 때무넹, 각 숫자에 해당하는 글자를 가져와 배열로 만든다. 그리고 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n",
    "  * **ex) ['l', 'o', 'v', 'e'] 를 입력하게 되면, 디코더의 입력 크기만큼 출력되므로 ['사', '랑', 'E', 'E'] 이 출력된다. 그러므로 'E'부터 그 뒤쪽을 제거하고 문자열로 만들어야만 '사랑' 이라는 단어가 나오게 된다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word):\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    \n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    prediction = tf.argmax(model, 2)\n",
    "    \n",
    "    result = sess.run(prediction,\n",
    "                     feed_dict={enc_input: input_batch,\n",
    "                               dec_input: output_batch,\n",
    "                               targets: target_batch})\n",
    "    decoded = [cahr_arr[i] for i in result[0]]\n",
    "    \n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 단어 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 번역 테스트 ===\n",
      "word ->  단어\n",
      "game ->  놀이\n",
      "girl ->  소녀\n",
      "love ->  사랑\n"
     ]
    }
   ],
   "source": [
    "print('\\n=== 번역 테스트 ===')\n",
    "\n",
    "print('word -> ', translate('word'))\n",
    "print('game -> ', translate('game'))\n",
    "print('girl -> ', translate('girl'))\n",
    "print('love -> ', translate('love'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
